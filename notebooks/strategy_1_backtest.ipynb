{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0278d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from polygon import RESTClient\n",
    "import datetime as dt\n",
    "\n",
    "import factor_analysis_functions\n",
    "import stock_data_functions\n",
    "import ewma_beta_tuning\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "from stock_data_functions import TickerComparison\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "import requests \n",
    "import time\n",
    "import json\n",
    "import seaborn as sns\n",
    "import math\n",
    "import importlib\n",
    "importlib.reload(stock_data_functions)\n",
    "importlib.reload(factor_analysis_functions)\n",
    "importlib.reload(ewma_beta_tuning)\n",
    "\n",
    "\n",
    "client = RESTClient('tt2gOLH0fHAmPX70a4QURLFy59PRCZr3')\n",
    "API_key = 'tt2gOLH0fHAmPX70a4QURLFy59PRCZr3'\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "from fredapi import Fred\n",
    "fred_api = Fred('e48d0413b1cd0a3b30b58d42225373de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9132f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_per_day(df):\n",
    "    df = df.copy()\n",
    "    df['time'] = df.index.strftime('%H:%M')\n",
    "\n",
    "    return df.groupby('time').count()\n",
    "\n",
    "def select_time(df):\n",
    "    t = (df.index.time > dt.time(9,30)) & (df.index.time < dt.time(16,30))\n",
    "\n",
    "    return df[t]\n",
    "\n",
    "def test(df):\n",
    "    df = df.copy()\n",
    "    df = df.isnull().sum()\n",
    "    return df\n",
    "\n",
    "def missing_by_time(df):\n",
    "    df = df.copy()\n",
    "    df['time'] = df.index.strftime('%H:%M')\n",
    "    df = df.groupby('time').apply(test) / df.groupby('time').agg(len)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b350c",
   "metadata": {},
   "source": [
    "### Backtest 1\n",
    "\n",
    "Idea :\n",
    "- If we see e.g., 5% + move intraday, does it revert? Does it revert in post market? \n",
    "- Categorise based on news\n",
    "- Categorise based on market cap\n",
    "- Categorise based on volume etc\n",
    "- Adjust for beta - calculate EWMA beta signal adjusted\n",
    "\n",
    "Time frame : last 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bb897",
   "metadata": {},
   "source": [
    "##### Initialise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "452cef29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'factor_analysis_functions' from '/Users/phillip/Desktop/Moon2/factor_analysis_functions.py'>"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import factor_analysis_functions\n",
    "import importlib\n",
    "importlib.reload(factor_analysis_functions)\n",
    "from factor_analysis_functions import run_full_pipeline, rolling_r2_from_intraday, _drop_weekends_index, _select_ext_hours_index, run_full_pipeline_multifactor_r2\n",
    "importlib.reload(factor_analysis_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clean_experiment(\n",
    "    *,\n",
    "    filing_date_gte: str,\n",
    "    stock: List[str],\n",
    "    regressor: List[str],                 # e.g., [\"SPY\"] or [\"SPY\",\"I:NDX\"]; we use regressor[0] as the factor\n",
    "    dt_updated: bool = False,\n",
    "    dt_updated_reg: bool = False,\n",
    "    # regime lookbacks\n",
    "    short_lookback: int = 20,\n",
    "    long_lookback: int = 60,\n",
    "    # residual z-scoring\n",
    "    sigma_lookback_days: int = 20,        # days for time-of-day residual std\n",
    "    event_windows: Tuple[int, ...] = (5, 10, 15),   # m-minute windows for agg residuals / z\n",
    "    # minute fetch knobs (pass-through to your pipeline)\n",
    "    minute_waiting_time: int = 60,\n",
    "    minute_chunksize: int = 200,\n",
    "    minute_fetch_in_chunks: bool = False,\n",
    "    # optional date bounds\n",
    "    daily_start_date: Optional[str] = None,\n",
    "    daily_end_date: Optional[str] = None,\n",
    "    minute_start_date: Optional[str] = None,\n",
    "    minute_end_date: Optional[str] = None,\n",
    "    use_optimal_betas: bool = True,\n",
    "    optimal_beta_kwargs: Dict[str, str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sets up and runs a clean experiment for multifactor intraday R² analysis.\n",
    "    \"\"\"\n",
    "    # ----------------------- 0) Run your pipeline once -----------------------\n",
    "    pipe = run_full_pipeline_multifactor_r2(\n",
    "        filing_date_gte=filing_date_gte,\n",
    "        dt_updated=dt_updated,\n",
    "        dt_updated_reg=dt_updated_reg,\n",
    "        stock=stock,\n",
    "        regressor=regressor,\n",
    "        lookback_long=long_lookback, \n",
    "        lookback_short=short_lookback, \n",
    "        minute_waiting_time=minute_waiting_time,\n",
    "        minute_chunksize=minute_chunksize,\n",
    "        minute_fetch_in_chunks=minute_fetch_in_chunks,\n",
    "        daily_start_date=daily_start_date,\n",
    "        minute_start_date=minute_start_date,\n",
    "        daily_end_date=daily_end_date,\n",
    "        minute_end_date=minute_end_date,\n",
    "        use_optimal_betas = use_optimal_betas,\n",
    "        optimal_beta_kwargs = optimal_beta_kwargs\n",
    "    )\n",
    "\n",
    "    # Convenience handles\n",
    "    stock_min_prices = pipe[\"stock_object_minute\"].tickers_stocks_prices     # MultiIndex: (ticker, field)\n",
    "    reg_min_prices   = pipe[\"regressor_object_minute\"].tickers_stocks_prices\n",
    "    stock_rets_min   = pipe[\"stock_returns_minute\"]                          # minute close log returns\n",
    "    reg_rets_min     = pipe[\"regressor_returns_minute\"]\n",
    "    stock_rets_day   = pipe[\"stock_returns_daily\"]                           # daily close log returns\n",
    "    reg_rets_day     = pipe[\"regressor_returns_daily\"]\n",
    "    beta_daily       = pipe[\"betas_df\"]                                      # EWMA betas (daily) vs factor_ticker\n",
    "\n",
    "    # Minute factor return (Series)\n",
    "    factor_min_ret = reg_rets_min.copy()\n",
    "\n",
    "    # ----------------------- 1) Minute meta: trade_date & time-of-day -----------------------\n",
    "    # Ensure tz-aware US/Eastern minute index\n",
    "    min_index = stock_rets_min.index\n",
    "    if getattr(min_index, \"tz\", None) is None:\n",
    "        # if not tz-aware, assume US/Eastern per your invariants\n",
    "        min_index = min_index.tz_localize(\"US/Eastern\")\n",
    "        stock_rets_min.index = min_index\n",
    "        reg_rets_min.index   = min_index\n",
    "        factor_min_ret.index = min_index\n",
    "    td_series = pd.Series(min_index.tz_convert(\"US/Eastern\").date, index=min_index, name=\"trade_date\")\n",
    "    tod_series = pd.Series(min_index.tz_convert(\"US/Eastern\").strftime(\"%H:%M\"), index=min_index, name=\"tod\")\n",
    "\n",
    "    return {'stock_min_prices' : stock_min_prices,\n",
    "            'reg_min_prices'   : reg_min_prices,\n",
    "            'stock_rets_min'   : stock_rets_min,\n",
    "            'reg_rets_min'     : reg_rets_min,\n",
    "            'stock_rets_day'   : stock_rets_day,\n",
    "            'reg_rets_day'     : reg_rets_day,\n",
    "            'beta_daily'       : beta_daily,\n",
    "            'factor_min_ret'   : factor_min_ret,\n",
    "            'td_series'        : td_series,\n",
    "            'tod_series'       : tod_series,\n",
    "            'min_index'        : min_index,\n",
    "            'pipe'             : pipe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1680,
   "id": "a858ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/13] META: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): META_5minute_2023-09_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 META\n",
      "[1/13] META: done\n",
      "[2/13] MU: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): MU_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 MU\n",
      "[2/13] MU: done\n",
      "[3/13] ALAB: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): ALAB_5minute_2024-03_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 ALAB\n",
      "[3/13] ALAB: done\n",
      "[4/13] TER: start\n",
      "last date saved :  2025-11-03 19:30:00-05:00\n",
      "Minute level data loaded from CSV (earliest): TER_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:30:00-05:00\n",
      "2025-11-03 19:30:00-05:00 TER\n",
      "[4/13] TER: done\n",
      "[5/13] CRDO: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): CRDO_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 CRDO\n",
      "[5/13] CRDO: done\n",
      "[6/13] AMKR: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): AMKR_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 AMKR\n",
      "[6/13] AMKR: done\n",
      "[7/13] AMD: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): AMD_5minute_2023-09_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 AMD\n",
      "[7/13] AMD: done\n",
      "[8/13] NVDA: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): NVDA_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 NVDA\n",
      "[8/13] NVDA: done\n",
      "[9/13] INTC: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): INTC_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 INTC\n",
      "[9/13] INTC: done\n",
      "[10/13] MSFT: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): MSFT_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 MSFT\n",
      "[10/13] MSFT: done\n",
      "[11/13] AAPL: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): AAPL_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 AAPL\n",
      "[11/13] AAPL: done\n",
      "[12/13] GOOG: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): GOOG_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 GOOG\n",
      "[12/13] GOOG: done\n",
      "[13/13] AAPL: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): AAPL_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 AAPL\n",
      "[13/13] AAPL: done\n",
      "[1/6] SPY: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): SPY_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 SPY\n",
      "[1/6] SPY: done\n",
      "[2/6] SMH: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): SMH_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 SMH\n",
      "[2/6] SMH: done\n",
      "[3/6] QQQ: start\n",
      "last date saved :  2025-11-03 19:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): QQQ_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 19:55:00-05:00\n",
      "2025-11-03 19:55:00-05:00 QQQ\n",
      "[3/6] QQQ: done\n",
      "[4/6] SPLV: start\n",
      "last date saved :  2025-11-03 16:05:00-05:00\n",
      "Minute level data loaded from CSV (earliest): SPLV_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 16:05:00-05:00\n",
      "2025-11-03 16:05:00-05:00 SPLV\n",
      "[4/6] SPLV: done\n",
      "[5/6] QVAL: start\n",
      "last date saved :  2025-11-03 15:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): QVAL_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 15:55:00-05:00\n",
      "2025-11-03 15:55:00-05:00 QVAL\n",
      "[5/6] QVAL: done\n",
      "[6/6] SRVR: start\n",
      "last date saved :  2025-11-03 15:55:00-05:00\n",
      "Minute level data loaded from CSV (earliest): SRVR_5minute_2023-11_minute_level_data.csv\n",
      "saving because we have the file and we do not want to update date : 2025-11-03 15:55:00-05:00\n",
      "2025-11-03 15:55:00-05:00 SRVR\n",
      "[6/6] SRVR: done\n",
      "[1/13] META: start\n",
      "[1/13] META: done\n",
      "[2/13] MU: start\n",
      "[2/13] MU: done\n",
      "[3/13] ALAB: start\n",
      "[3/13] ALAB: done\n",
      "[4/13] TER: start\n",
      "[4/13] TER: done\n",
      "[5/13] CRDO: start\n",
      "[5/13] CRDO: done\n",
      "[6/13] AMKR: start\n",
      "[6/13] AMKR: done\n",
      "[7/13] AMD: start\n",
      "[7/13] AMD: done\n",
      "[8/13] NVDA: start\n",
      "[8/13] NVDA: done\n",
      "[9/13] INTC: start\n",
      "[9/13] INTC: done\n",
      "[10/13] MSFT: start\n",
      "[10/13] MSFT: done\n",
      "[11/13] AAPL: start\n",
      "[11/13] AAPL: done\n",
      "[12/13] GOOG: start\n",
      "[12/13] GOOG: done\n",
      "[13/13] AAPL: start\n",
      "[13/13] AAPL: done\n",
      "[1/6] SPY: start\n",
      "[1/6] SPY: done\n",
      "[2/6] SMH: start\n",
      "[2/6] SMH: done\n",
      "[3/6] QQQ: start\n",
      "[3/6] QQQ: done\n",
      "[4/6] SPLV: start\n",
      "[4/6] SPLV: done\n",
      "[5/6] QVAL: start\n",
      "[5/6] QVAL: done\n",
      "[6/6] SRVR: start\n",
      "[6/6] SRVR: done\n",
      "[tune] META ...\n",
      "[tune] MU ...\n",
      "[tune] ALAB ...\n",
      "[tune] TER ...\n",
      "[tune] CRDO ...\n",
      "[tune] AMKR ...\n",
      "[tune] AMD ...\n",
      "[tune] NVDA ...\n",
      "[tune] INTC ...\n",
      "[tune] MSFT ...\n",
      "[tune] AAPL ...\n",
      "[tune] GOOG ...\n",
      "ridge\n",
      "0.1 0.31451300202509513 thresh, rel\n",
      "ridge\n",
      "0.1 0.9752369534033414 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.03059128676514384 thresh, rel\n",
      "0.99\n",
      "0.1 0.9336625811547878 thresh, rel\n",
      "ridge\n",
      "0.1 0.6598281256237902 thresh, rel\n",
      "ridge\n",
      "0.1 0.9926287670452915 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.03353587895867716 thresh, rel\n",
      "0.99\n",
      "0.1 0.867019687851791 thresh, rel\n",
      "ridge\n",
      "0.1 0.44204732472387015 thresh, rel\n",
      "ridge\n",
      "0.1 0.9920381375059036 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.042682757629486444 thresh, rel\n",
      "0.99\n",
      "0.1 0.48348727805832703 thresh, rel\n",
      "ridge\n",
      "0.1 0.49018421349444863 thresh, rel\n",
      "ridge\n",
      "0.1 0.9939547846451156 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.02855368986364497 thresh, rel\n",
      "0.99\n",
      "0.1 0.9167307048731456 thresh, rel\n",
      "ridge\n",
      "0.1 0.8381409848169593 thresh, rel\n",
      "ridge\n",
      "0.1 0.994689542482468 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.03333012076497476 thresh, rel\n",
      "0.99\n",
      "0.1 0.8881980138570025 thresh, rel\n",
      "ridge\n",
      "0.1 0.41761678487557974 thresh, rel\n",
      "ridge\n",
      "0.1 0.9410709904277443 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.033842017456537234 thresh, rel\n",
      "0.99\n",
      "0.1 0.8952695853707641 thresh, rel\n",
      "ridge\n",
      "0.1 0.478234465616012 thresh, rel\n",
      "ridge\n",
      "0.1 0.9427390042861226 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.96\n",
      "0.1 0.027416517168154266 thresh, rel\n",
      "0.99\n",
      "0.1 0.9170792259064031 thresh, rel\n",
      "ridge\n",
      "0.1 0.8337758543401579 thresh, rel\n",
      "ridge\n",
      "0.1 0.9975481929340289 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.96\n",
      "0.1 0.023416934507521168 thresh, rel\n",
      "0.99\n",
      "0.1 0.879353907287631 thresh, rel\n",
      "ridge\n",
      "0.1 0.9415822534706444 thresh, rel\n",
      "ridge\n",
      "0.1 0.9976524993896251 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.99\n",
      "0.1 0.06258436443075473 thresh, rel\n",
      "0.99\n",
      "0.1 0.8428328902478882 thresh, rel\n",
      "ridge\n",
      "0.1 0.8012208785784838 thresh, rel\n",
      "ridge\n",
      "0.1 0.9969374254054183 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.04077600532812157 thresh, rel\n",
      "0.99\n",
      "0.1 0.8270827988263915 thresh, rel\n",
      "ridge\n",
      "0.1 0.6375273905360672 thresh, rel\n",
      "ridge\n",
      "0.1 0.995075796264095 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.94\n",
      "0.1 0.03951277316465691 thresh, rel\n",
      "0.99\n",
      "0.1 0.8712679232404571 thresh, rel\n",
      "ridge\n",
      "0.1 0.661523156688474 thresh, rel\n",
      "ridge\n",
      "0.1 0.9952541415006936 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.98\n",
      "0.1 0.04774659533103114 thresh, rel\n",
      "0.99\n",
      "0.1 0.8818652513467621 thresh, rel\n",
      "ridge\n",
      "0.1 0.6375273905360672 thresh, rel\n",
      "ridge\n",
      "0.1 0.995075796264095 thresh, rel\n",
      "ridge\n",
      "0.1 0.0 thresh, rel\n",
      "0.94\n",
      "0.1 0.03951277316465691 thresh, rel\n",
      "0.99\n",
      "0.1 0.8712679232404571 thresh, rel\n",
      "[1/1] META: start\n",
      "[1/1] META: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] MU: start\n",
      "[1/1] MU: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] ALAB: start\n",
      "[1/1] ALAB: done\n",
      "[1/1] SMH: start\n",
      "[1/1] SMH: done\n",
      "[1/1] TER: start\n",
      "[1/1] TER: done\n",
      "[1/2] SPY: start\n",
      "[1/2] SPY: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] CRDO: start\n",
      "[1/1] CRDO: done\n",
      "[1/2] SMH: start\n",
      "[1/2] SMH: done\n",
      "[2/2] SPLV: start\n",
      "[2/2] SPLV: done\n",
      "[1/1] AMKR: start\n",
      "[1/1] AMKR: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] AMD: start\n",
      "[1/1] AMD: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] NVDA: start\n",
      "[1/1] NVDA: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] INTC: start\n",
      "[1/1] INTC: done\n",
      "[1/2] SMH: start\n",
      "[1/2] SMH: done\n",
      "[2/2] SPLV: start\n",
      "[2/2] SPLV: done\n",
      "[1/1] MSFT: start\n",
      "[1/1] MSFT: done\n",
      "[1/1] QQQ: start\n",
      "[1/1] QQQ: done\n",
      "[1/1] AAPL: start\n",
      "[1/1] AAPL: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n",
      "[1/1] GOOG: start\n",
      "[1/1] GOOG: done\n",
      "[1/2] QQQ: start\n",
      "[1/2] QQQ: done\n",
      "[2/2] SMH: start\n",
      "[2/2] SMH: done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phillip/Desktop/Moon2/factor_analysis_functions.py:324: FutureWarning:\n",
      "\n",
      "The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regressors and Stocks \n",
    "regressor                   = ['SPY','SMH','QQQ','SPLV', 'QVAL', 'SRVR']\n",
    "stock                       = ['META', 'MU','ALAB', 'TER', 'CRDO', 'AMKR', 'AMD', 'NVDA', 'INTC', 'MSFT', 'AAPL', 'GOOG', 'AAPL']\n",
    "\n",
    "# Data Pulling\n",
    "daily_start_date            = (dt.datetime.today() - pd.Timedelta(weeks=104)).strftime('%Y-%m-%d')\n",
    "dt_updated_reg              = False\n",
    "dt_updated                  = False\n",
    "\n",
    "minute_waiting_time     = 30\n",
    "minute_chunksize        = 200\n",
    "minute_fetch_in_chunks  = True\n",
    "\n",
    "daily_start_date        = '2023-11-01'\n",
    "minute_start_date       = '2023-11-01'\n",
    "daily_end_date          = '2025-11-03'\n",
    "minute_end_date         = '2025-11-03'\n",
    "\n",
    "# Beta Tuning\n",
    "factor_sets = [\n",
    "    ['SPY'], ['QQQ'], ['SMH'], ['SRVR'],\n",
    "    ['SPY', 'SMH'], ['QQQ', 'SMH'],\n",
    "    ['QQQ', 'SRVR'], ['SPY', 'SRVR'],\n",
    "    ['SPY', 'QVAL', 'SMH'],\n",
    "    ['QQQ', 'QVAL', 'SMH'],\n",
    "    ['QQQ', 'SPLV'], ['SMH', 'SPLV'],\n",
    "    ['SPLV', 'SRVR'], ['SPLV', 'QVAL','SRVR']\n",
    "]\n",
    "\n",
    "\n",
    "ridge_grid  = [1e-8, 1e-6, 1e-4, 1e-3]\n",
    "lambdas     = [0.90, 0.94, 0.96, 0.98, 0.99]\n",
    "thresh      = 0.1                       # Threshold for parameter tuning where e.g., 10% difference in pseudo OOS MSE is treated as the same\n",
    "# Idiosyncratic Return Params\n",
    "warmup                  = 30 \n",
    "short_lookback          = 5\n",
    "long_lookback           = 20\n",
    "sigma_lookback_days     = 5\n",
    "event_windows           = (1, 3, 12, 24)\n",
    "\n",
    "optimal_beta_kwargs = {\n",
    "    'factor_sets': factor_sets,\n",
    "    'lambdas': lambdas,\n",
    "    'thresh': thresh,\n",
    "    'ridge_grid':ridge_grid,\n",
    "    'daily_start_date':daily_start_date,\n",
    "    'date_updated':dt_updated,\n",
    "    'date_updated_regressor':dt_updated_reg,\n",
    "}\n",
    "\n",
    "data = setup_clean_experiment(\n",
    "    filing_date_gte=\"2023-09-01\",\n",
    "    stock=stock,\n",
    "    regressor=regressor,\n",
    "    dt_updated=dt_updated,\n",
    "    dt_updated_reg=dt_updated_reg,\n",
    "    short_lookback=short_lookback,\n",
    "    long_lookback=long_lookback,\n",
    "    sigma_lookback_days=sigma_lookback_days,\n",
    "    event_windows=event_windows,\n",
    "    minute_waiting_time=minute_waiting_time,\n",
    "    minute_chunksize=minute_chunksize,\n",
    "    minute_fetch_in_chunks=minute_fetch_in_chunks,\n",
    "    optimal_beta_kwargs=optimal_beta_kwargs,\n",
    "    minute_end_date=minute_end_date,\n",
    "    daily_end_date=daily_end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "26bf469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clean_experiment(\n",
    "        stock            = stock,\n",
    "        factor_ticker    = 'SPY',\n",
    "        stock_min_prices = data['stock_min_prices'],\n",
    "        reg_min_prices   = data['reg_min_prices'],\n",
    "        stock_rets_min   = data['stock_rets_min'],\n",
    "        reg_rets_min     = data['reg_rets_min'],\n",
    "        stock_rets_day   = data['stock_rets_day'],\n",
    "        reg_rets_day     = data['reg_rets_day'],\n",
    "        beta_daily       = data['beta_daily'],\n",
    "        factor_min_ret   = data['factor_min_ret'],\n",
    "        td_series        = data['td_series'],\n",
    "        tod_series       = data['tod_series'],\n",
    "        min_index        = data['min_index'],\n",
    "        pipe             = data['pipe']\n",
    "):\n",
    "   # ----------------------- 2) Lagged betas (no look-ahead) & minute mapping -----------------------\n",
    "    beta_daily_lag1 = beta_daily.shift(1)  # strict D-1\n",
    "    # Map D-1 betas to minutes by trade_date\n",
    "    beta_minute_lag1 = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "    for s in stock:\n",
    "        m = td_series.map(beta_daily_lag1[s])\n",
    "        beta_minute_lag1[s] = m.values\n",
    "\n",
    "    # ----------------------- 3) Minute residuals (stock - beta_{D-1} * factor) -----------------------\n",
    "    residual_1m = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "    for s in stock:\n",
    "        residual_1m[s] = stock_rets_min[s] - beta_minute_lag1[s] * factor_min_ret\n",
    "\n",
    "    # ----------------------- 4) m-minute residual sums & explained-share -----------------------\n",
    "    def _roll_sum(df: pd.DataFrame, m: int) -> pd.DataFrame:\n",
    "        return df.rolling(m, min_periods=m).sum()\n",
    "\n",
    "    residual_m: Dict[int, pd.DataFrame] = {m: _roll_sum(residual_1m, m) for m in event_windows}\n",
    "    # For explained share, need stock m-min move and factor m-min move\n",
    "    stock_m = {m: _roll_sum(stock_rets_min, m) for m in event_windows}\n",
    "    factor_m = {m: _roll_sum(factor_min_ret.to_frame(\"f\"), m)[\"f\"] for m in event_windows}\n",
    "\n",
    "    explained_share_m: Dict[int, pd.DataFrame] = {}\n",
    "    for m in event_windows:\n",
    "        es = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "        fmove = factor_m[m]\n",
    "        for s in stock:\n",
    "            # predicted = beta_{D-1} * factor move (use minute-mapped beta)\n",
    "            pred = beta_minute_lag1[s] * fmove\n",
    "            actual = stock_m[m][s]\n",
    "            num = pred.abs()\n",
    "            den = actual.abs().replace(0, np.nan)\n",
    "            es[s] = (num / den).clip(0.0, 1.0)\n",
    "        explained_share_m[m] = es\n",
    "\n",
    "    # ----------------------- 5) Time-of-day sigma for residual m-min (lagged, no leakage) ---------\n",
    "    # Helper to compute lagged TOD std by minute bucket across past N days\n",
    "    def _tod_sigma_lagged(x: pd.Series, td: pd.Series, tod: pd.Series, window_days: int) -> pd.Series:\n",
    "        df = pd.DataFrame({\"val\": x, \"td\": td, \"tod\": tod})\n",
    "        df = df.dropna(subset=[\"val\"])\n",
    "        # sort by (tod, trade_date) so each minute-of-day series is ordered by day\n",
    "        df = df.sort_values([\"tod\", \"td\"])\n",
    "        # rolling std over past N days PER time-of-day, using shift(1) to avoid using same-day value\n",
    "        grp = df.groupby(\"tod\", sort=False)[\"val\"]\n",
    "        sigma = grp.apply(lambda s: s.shift(1).rolling(window_days, min_periods=max(5, window_days//2)).std())\n",
    "        # restore original index order\n",
    "        df[\"sigma\"] = sigma.values\n",
    "        df = df.sort_index()\n",
    "        out = pd.Series(index=x.index, dtype=float)\n",
    "        out.loc[df.index] = df[\"sigma\"].values\n",
    "        return out\n",
    "\n",
    "    sigma_tod_m: Dict[int, pd.DataFrame] = {}\n",
    "    z_m: Dict[int, pd.DataFrame] = {}\n",
    "    for m in event_windows:\n",
    "        # Compute sigma per stock independently\n",
    "        sig_df = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "        z_df   = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "        for s in stock:\n",
    "            rs = residual_m[m][s]\n",
    "            sig = _tod_sigma_lagged(rs, td_series, tod_series, sigma_lookback_days)\n",
    "            sig_df[s] = sig\n",
    "            z_df[s] = rs / sig\n",
    "        sigma_tod_m[m] = sig_df\n",
    "        z_m[m] = z_df\n",
    "\n",
    "    # ----------------------- 6) Daily regime features: R2 short/long (lagged) -----------------------\n",
    "    # Recompute R² (short & long) from your minute-labelled table to keep consistency\n",
    "    labelled_min = pipe[\"labelled_combined_returns_min\"]  # has 'trade_date' already\n",
    "    # short\n",
    "    r2_short_daily = rolling_r2_from_intraday(\n",
    "        labelled_min, beta_daily, lookback_days=short_lookback, factor_col=factor_ticker\n",
    "    )\n",
    "    # long\n",
    "    r2_long_daily = rolling_r2_from_intraday(\n",
    "        labelled_min, beta_daily, lookback_days=long_lookback, factor_col=factor_ticker\n",
    "    )\n",
    "    # Align index types & sort\n",
    "    r2_short_daily = r2_short_daily.sort_index()\n",
    "    r2_long_daily  = r2_long_daily.sort_index()\n",
    "    delta_daily    = (r2_short_daily - r2_long_daily).reindex_like(r2_short_daily)\n",
    "\n",
    "    r2_short_lag1 = r2_short_daily.shift(1)\n",
    "    delta_lag1    = delta_daily.shift(1)\n",
    "\n",
    "    # Map lagged regime features to each minute by trade_date (prior day values)\n",
    "    regime_short_minute = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "    regime_delta_minute = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "    td_no_tz = pd.Series(pd.to_datetime(td_series).values.astype(\"datetime64[D]\"), index=td_series.index)\n",
    "    for s in stock:\n",
    "        # map by date only\n",
    "        r2_map   = r2_short_lag1[s].copy()\n",
    "        r2_map.index = pd.to_datetime(r2_map.index).date\n",
    "        dlt_map  = delta_lag1[s].copy()\n",
    "        dlt_map.index = pd.to_datetime(dlt_map.index).date\n",
    "        regime_short_minute[s] = td_series.map(r2_map).values\n",
    "        regime_delta_minute[s] = td_series.map(dlt_map).values\n",
    "\n",
    "    # ----------------------- 7) Volume TOD z-scores (microstructure proxy) -----------------------\n",
    "    # Pull 5-min volume from minute price tables\n",
    "    vol_5m = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "    for s in stock:\n",
    "        vol_5m[s] = stock_min_prices[(s, \"volume\")].reindex(min_index)\n",
    "\n",
    "    def _tod_z_lagged(x: pd.Series, td: pd.Series, tod: pd.Series, window_days: int) -> pd.Series:\n",
    "        # like sigma, but z-score\n",
    "        df = pd.DataFrame({\"val\": x, \"td\": td, \"tod\": tod}).dropna(subset=[\"val\"])\n",
    "        df = df.sort_values([\"tod\", \"td\"])\n",
    "        grp = df.groupby(\"tod\", sort=False)[\"val\"]\n",
    "        mean_ = grp.apply(lambda s: s.shift(1).rolling(window_days, min_periods=max(5, window_days//2)).mean())\n",
    "        std_  = grp.apply(lambda s: s.shift(1).rolling(window_days, min_periods=max(5, window_days//2)).std())\n",
    "        df[\"z\"] = (df[\"val\"] - mean_.values) / std_.values\n",
    "        df = df.sort_index()\n",
    "        out = pd.Series(index=x.index, dtype=float)\n",
    "        out.loc[df.index] = df[\"z\"].values\n",
    "        return out\n",
    "\n",
    "    volume_z_tod = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "    for s in stock:\n",
    "        volume_z_tod[s] = _tod_z_lagged(vol_5m[s], td_series, tod_series, sigma_lookback_days)\n",
    "\n",
    "    # ----------------------- 8) Package & return -----------------------\n",
    "    return {\n",
    "        \"pipe\": pipe,\n",
    "        \"minute_meta\": {\"trade_date\": td_series, \"tod\": tod_series},\n",
    "        \"minute_stock_rets\": stock_rets_min,\n",
    "        \"minute_factor_ret\": factor_min_ret,\n",
    "        \"beta_daily\": beta_daily,\n",
    "        \"beta_daily_lag1\": beta_daily_lag1,\n",
    "        \"beta_minute_lag1\": beta_minute_lag1,\n",
    "        \"residual_1m\": residual_1m,\n",
    "        \"residual_m\": residual_m,\n",
    "        \"sigma_tod_m\": sigma_tod_m,\n",
    "        \"z_m\": z_m,\n",
    "        \"explained_share_m\": explained_share_m,\n",
    "        \"volume_5m\": vol_5m,\n",
    "        \"volume_z_tod\": volume_z_tod,\n",
    "        \"regime_daily\": {\n",
    "            \"r2_short_daily\": r2_short_daily,\n",
    "            \"r2_long_daily\":  r2_long_daily,\n",
    "            \"delta_daily\":    delta_daily,\n",
    "            \"r2_short_lag1\":  r2_short_lag1,\n",
    "            \"delta_lag1\":     delta_lag1,\n",
    "        },\n",
    "        \"regime_minute_lag1\": {\n",
    "            \"r2_short\": regime_short_minute,\n",
    "            \"delta\":    regime_delta_minute,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3468065",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_daily      = data['beta_daily'].copy()\n",
    "td_series       = data['td_series'].copy()\n",
    "min_index       = data['min_index'].copy()\n",
    "stock_rets_min  = data['stock_rets_min'].copy()\n",
    "factor_min_ret  = data['factor_min_ret'].copy()\n",
    "tod_series      = data['tod_series'].copy()\n",
    "\n",
    "# Filter out weekends and times outside when market is open\n",
    "min_index            = _drop_weekends_index(min_index)\n",
    "min_index            = _select_ext_hours_index(min_index)\n",
    "factor_min_ret_index = _drop_weekends_index(factor_min_ret.index)\n",
    "factor_min_ret_index = _select_ext_hours_index(factor_min_ret_index)\n",
    "\n",
    "# Filter out when markets are closed\n",
    "tod_series     = tod_series[min_index].copy()\n",
    "td_series      = td_series[min_index].copy()\n",
    "factor_min_ret = factor_min_ret.loc[factor_min_ret_index].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1eaaeb",
   "metadata": {},
   "source": [
    "##### Create beta and idioscryntaic matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Single Beta Version (Not Used) -----------------------\n",
    "# beta_daily_lag1 = beta_daily.shift(1)  # strict D-1\n",
    "# # Map D-1 betas to minutes by trade_date\n",
    "# beta_minute_lag1 = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "# for s in stock:\n",
    "#     m = td_series.map(beta_daily_lag1[s])\n",
    "#     beta_minute_lag1[s] = m.values\n",
    "\n",
    "# # ----------------------- 3) Minute residuals (stock - beta_{D-1} * factor) -----------------------\n",
    "# residual_1m = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "# for s in stock:\n",
    "#     if s not in stock_rets_min.columns or stock_rets_min[s].isnull().all():\n",
    "#         residual_1m.drop(columns=[s], inplace=True)\n",
    "#         stock.remove(s)\n",
    "#         print(f'Stock {s} : Data not loaded properly - removing from backtest')\n",
    "#     else:\n",
    "#         residual_1m[s] = stock_rets_min[s] - beta_minute_lag1[s] * factor_min_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to shift betas back by 1 Day, then assign D-1 betas to each 5 minute interval by trade_date\n",
    "beta_daily_lag1 = beta_daily.shift(1)  \n",
    "# Map D-1 betas to minutes by trade_date\n",
    "beta_minute_lag1 = pd.DataFrame(index=min_index, columns=stock, dtype=float)\n",
    "beta_minute_lag1 = {}\n",
    "for s in stock:\n",
    "    # go by each stock\n",
    "    cols                  = beta_daily_lag1[s].columns\n",
    "    beta_daily_lag1_stock = beta_daily_lag1[s].copy()\n",
    "\n",
    "    beta_daily_lag1        = beta_daily.shift(1)  # strict D-1\n",
    "    beta_minute_lag1_stock = pd.DataFrame(index=min_index, columns=cols, dtype=float)\n",
    "    for i in cols:\n",
    "        # Assign each regressor to the D-1 trade date\n",
    "        m = td_series.map(beta_daily_lag1_stock[i])\n",
    "        beta_minute_lag1_stock[i] = m.values\n",
    "\n",
    "    beta_minute_lag1[s] = beta_minute_lag1_stock.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "id": "db74a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Calculate residuals\n",
    "def calc_idio_rets(s, *, stock_rets_min, factor_min_ret, beta_minute_lag1, dtype='float32'):\n",
    "    # stock returns: drop missing upfront\n",
    "    Sr    = stock_rets_min[s].dropna().astype(dtype)\n",
    "\n",
    "    # betas: drop any row with any missing leg\n",
    "    B_all = beta_minute_lag1[s]\n",
    "    B     = B_all.dropna(how='any')\n",
    "\n",
    "    # factors: use only the columns this stock actually uses, then align to B's minutes and drop rows with any NaN\n",
    "    cols = B.columns.intersection(factor_min_ret.columns)\n",
    "    F    = factor_min_ret[cols].reindex(B.index).dropna(how='any')\n",
    "\n",
    "    # final time index = minutes present (and fully observed) in B, F, and Sr\n",
    "    idx = B.index.intersection(F.index, sort=False).intersection(Sr.index, sort=False)\n",
    "\n",
    "    B = B.loc[idx, cols].astype(dtype)\n",
    "    F = F.loc[idx, cols].astype(dtype)\n",
    "    Sr = Sr.loc[idx].astype(dtype)\n",
    "    # row-wise dot for factor contribution; no fills anywhere\n",
    "    fac = np.einsum('ij,ij->i', B.to_numpy(copy=False), F.to_numpy(copy=False), optimize=True)\n",
    "    fac = pd.Series(fac, index=idx, dtype=dtype)\n",
    "    resid = Sr - fac\n",
    "    return resid, fac\n",
    "\n",
    "# single pass to build both outputs (no double compute)\n",
    "results = {s: calc_idio_rets(s,\n",
    "                             stock_rets_min=stock_rets_min,\n",
    "                             factor_min_ret=factor_min_ret,\n",
    "                             beta_minute_lag1=beta_minute_lag1)\n",
    "           for s in stock}\n",
    "\n",
    "residual_1m = pd.DataFrame({s: r[0] for s, r in results.items()})\n",
    "fac_rets    = pd.DataFrame({s: r[1] for s, r in results.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "id": "44239a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _roll_sum(df: pd.DataFrame, m: int) -> pd.DataFrame:\n",
    "    return df.rolling(m, min_periods=m).sum()\n",
    "\n",
    "stocks_aligned = stock_rets_min[residual_1m.columns].reindex(residual_1m.index)\n",
    "\n",
    "residual_m: Dict[int, pd.DataFrame] = {m: _roll_sum(residual_1m, m) for m in event_windows}\n",
    "stock_m:    Dict[int, pd.DataFrame] = {m: _roll_sum(stocks_aligned, m) for m in event_windows}\n",
    "\n",
    "factor_1m = stocks_aligned - residual_1m\n",
    "factor_m:  Dict[int, pd.DataFrame] = {m: _roll_sum(factor_1m, m) for m in event_windows}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18351277",
   "metadata": {},
   "source": [
    "#### Idiosyncratic Vol & Diagnostics - Event Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "efa7cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import factor_analysis_functions\n",
    "import importlib\n",
    "importlib.reload(factor_analysis_functions)\n",
    "from factor_analysis_functions import sanity_check_sigma_and_z, build_sigma_and_z_from_tod, z_exceedance_diagnostic, plot_hourly_net_resid_for_exceedance_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfe2ae",
   "metadata": {},
   "source": [
    "Idio syncratic z score prior days diurnal approach:\n",
    "- Idea is to get a score that sees how much of the stock's returns are explained by the regressor\n",
    "- Normalize by the rolling standard deviation of idiosyncratic returns at that time of the day (e.g., 9:30-9:35 past m days)\n",
    "- in the _tod_sigma_lagged function, we require either half the window length or 5 observations to form an actual sigma observation\n",
    "- The tod_sigma_lagged function gets you the volatility to normalize - but uses the volatility of the hour not of 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "id": "f77bfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_tod_m, z_m = build_sigma_and_z_from_tod(\n",
    "    residual_m=residual_m,\n",
    "    td_series=td_series,\n",
    "    tod_series=tod_series,\n",
    "    stocks=stock,              \n",
    "    window_days=sigma_lookback_days\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1aebed",
   "metadata": {},
   "source": [
    "This checks the z score anomalies e.g., Z > 10. This groups by\n",
    "- Session (pre post regular) and ticker and date\n",
    "- Plots where this happens\n",
    "- Provides table of the top 10 occurance of these deviations\n",
    "- This is idiosyncratic movements\n",
    "- Count is number of times we had something exceed threshold in that session\n",
    "- z_avg is the average of the abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "id": "83344212",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "out = z_exceedance_diagnostic(\n",
    "    sigma_tod_m=sigma_tod_m,\n",
    "    z_m=z_m,\n",
    "    residual_m=residual_m,\n",
    "    m=m,                   \n",
    "    Z=3.0,\n",
    "    top_k=10,\n",
    "    return_plot=False,\n",
    "    integrity_atol=1e-2,\n",
    "    integrity_rtol=1e-2\n",
    ")\n",
    "\n",
    "summary     = out[\"summary_wide\"].round(2)        # MultiIndex columns: ticker → ['date','session','z_avg','z_count']\n",
    "reversion   = out[\"reversion_wide\"]    # ticker → ['date','session','spread','count_above_spread','z_count']\n",
    "events      = out[\"events_long\"]          # per-bar exceedances with z, sigma, resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "603f6de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "8a29f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = plot_hourly_net_resid_for_exceedance_hours(\n",
    "    sigma_tod_m=sigma_tod_m,\n",
    "    z_m=z_m,\n",
    "    residual_m=residual_m,\n",
    "    m=1,\n",
    "    Z=3.0,\n",
    "    tickers=['AMD', 'NVDA'],   # or None to auto-pick top-2\n",
    "    session=None)\n",
    "fig = res[\"fig\"]\n",
    "pio.renderers.default = \"browser\"\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41418b",
   "metadata": {},
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "1a000a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alignment', 'sigma_summary', 'z_summary', 'z_by_hour', 'flags'])\n"
     ]
    }
   ],
   "source": [
    "from factor_analysis_functions import sanity_check_sigma_and_z\n",
    "importlib.reload(factor_analysis_functions)\n",
    "\n",
    "checks = sanity_check_sigma_and_z(sigma_tod_m=sigma_tod_m, z_m=z_m)\n",
    "print(checks[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a533b2",
   "metadata": {},
   "source": [
    "#### Regime Analysis : R2 plots with multi factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "cf0a1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'factor_analysis_functions' from '/Users/phillip/Desktop/Moon2/factor_analysis_functions.py'>"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analysis_functions import sanity_check_sigma_and_z, run_full_pipeline, rolling_r2_intraday_multifactor_perstock, run_full_pipeline_multifactor_r2\n",
    "importlib.reload(factor_analysis_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_interactive_r2_dashboard_multifactor(\n",
    "    *,\n",
    "    filing_date_gte: str,\n",
    "    stock: Sequence[str],\n",
    "    regressor: Sequence[str],\n",
    "    short_lookback: int = 20,\n",
    "    long_lookback: int = 60,\n",
    "    # fetch knobs\n",
    "    dt_updated: bool = False,\n",
    "    dt_updated_reg: bool = False,\n",
    "    minute_waiting_time: int = 60,\n",
    "    minute_chunksize: int = 200,\n",
    "    minute_fetch_in_chunks: bool = False,\n",
    "    daily_start_date: str | None = None,\n",
    "    daily_end_date: str | None = None,\n",
    "    minute_start_date: str | None = None,\n",
    "    minute_end_date: str | None = None,\n",
    "    # presentation\n",
    "    init_stock: str | None = None,\n",
    "    title: str | None = None,\n",
    "    optimal_beta_kwargs: Dict[str, Any] = None,\n",
    "):\n",
    "    # ---------- build data ----------\n",
    "    pipe = run_full_pipeline_multifactor_r2(\n",
    "        filing_date_gte=filing_date_gte,\n",
    "        stock=stock,\n",
    "        regressor=regressor,\n",
    "        lookback_short=short_lookback,\n",
    "        lookback_long=long_lookback,\n",
    "        dt_updated=dt_updated,\n",
    "        dt_updated_reg=dt_updated_reg,\n",
    "        minute_waiting_time=minute_waiting_time,\n",
    "        minute_chunksize=minute_chunksize,\n",
    "        minute_fetch_in_chunks=minute_fetch_in_chunks,\n",
    "        daily_start_date=daily_start_date,\n",
    "        daily_end_date=daily_end_date,\n",
    "        minute_start_date=minute_start_date,\n",
    "        minute_end_date=minute_end_date,\n",
    "        use_optimal_betas=True,\n",
    "        optimal_beta_kwargs=optimal_beta_kwargs,\n",
    "    )\n",
    "\n",
    "    r2s = pipe[\"r2_short_daily\"]   # index: trading_day, cols: tickers\n",
    "    r2l = pipe[\"r2_long_daily\"]\n",
    "\n",
    "    # ensure aligned index across both frames\n",
    "    idx = r2s.index.union(r2l.index).sort_values()\n",
    "    r2s = r2s.reindex(idx)\n",
    "    r2l = r2l.reindex(idx)\n",
    "\n",
    "    # ---------- figure ----------\n",
    "    fig = go.Figure()\n",
    "    tickers = list(r2s.columns)\n",
    "    init = init_stock or (tickers[0] if tickers else None)\n",
    "    if init is None:\n",
    "        return go.Figure()\n",
    "\n",
    "    # add 2 traces per stock, default hidden; show only init\n",
    "    for t in tickers:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=idx, y=r2s[t], mode=\"lines\", name=f\"{t} R² (short {short_lookback}d)\",\n",
    "            hovertemplate=\"%{x|%Y-%m-%d}<br>R²=%{y:.3f}<extra></extra>\",\n",
    "            visible=(t == init)\n",
    "        ))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=idx, y=r2l[t], mode=\"lines\", name=f\"{t} R² (long {long_lookback}d)\",\n",
    "            line=dict(dash=\"dash\"),  # dashed line for long lookback\n",
    "            hovertemplate=\"%{x|%Y-%m-%d}<br>R²=%{y:.3f}<extra></extra>\",\n",
    "            visible=(t == init)\n",
    "        ))\n",
    "\n",
    "    # dropdown to toggle stocks: flip visibility for the pair of traces belonging to that ticker\n",
    "    buttons = []\n",
    "    total = len(fig.data)\n",
    "    traces_per_stock = 2\n",
    "    for i, t in enumerate(tickers):\n",
    "        vis = [False] * total\n",
    "        base = i * traces_per_stock\n",
    "        vis[base] = True\n",
    "        vis[base + 1] = True\n",
    "        buttons.append(dict(\n",
    "            label=t,\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": vis},\n",
    "                  {\"title\": f\"{t} — Rolling multi-factor R² (short vs long)\"}],\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        updatemenus=[dict(\n",
    "            type=\"dropdown\", direction=\"down\", x=1.0, xanchor=\"right\", y=1.12, yanchor=\"top\",\n",
    "            buttons=buttons, showactive=True, bgcolor=\"white\", bordercolor=\"#ccc\", pad=dict(r=8, t=2, b=2, l=2),\n",
    "        )],\n",
    "        xaxis=dict(title=\"Trading day\"),\n",
    "        yaxis=dict(title=\"R²\", rangemode=\"tozero\"),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0),\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(l=70, r=70, t=70, b=60),\n",
    "        title=title or f\"{init} — Rolling multi-factor R² (short vs long)\"\n",
    "    )\n",
    "    pio.templates.default = \"plotly_white\"\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_interactive_r2_dashboard_multifactor(\n",
    "        filing_date_gte=\"2023-09-01\",\n",
    "        stock=stock,\n",
    "        regressor=['QQQ', 'SPY', 'SMH'],\n",
    "        short_lookback=short_lookback,\n",
    "        long_lookback=long_lookback,\n",
    "        dt_updated=dt_updated,\n",
    "        dt_updated_reg=dt_updated_reg,\n",
    "        minute_waiting_time=minute_waiting_time,\n",
    "        minute_chunksize=minute_chunksize,\n",
    "        minute_fetch_in_chunks=minute_fetch_in_chunks,\n",
    "        optimal_beta_kwargs=optimal_beta_kwargs,\n",
    "        minute_end_date = '2025-11-04'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "id": "37b95d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(98753) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "fig.show()  # pops a new browser tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14867d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
